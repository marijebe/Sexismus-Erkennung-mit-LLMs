# -*- coding: utf-8 -*-
"""Masterarbeit_MariJebe.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YiDyNGOlX-WxzEwQ_6KhsS_cXNMVoJAS

# **Sexismus-Erkennung mit Large Language Models**

# Imports & Setup
"""

!pip install -r requirements.txt

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from datasets import load_dataset, DatasetDict, Dataset, load_from_disk
import matplotlib.pyplot as plt
import tensorflow as tf
import os
from transformers import AutoTokenizer, AutoModel, TFAutoModel
from transformers import AutoModelForSequenceClassification,  AutoConfig
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, DistilBertModel
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm
import optuna
from collections import Counter
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
import seaborn as sns
import spacy
!python -m spacy download de_core_news_sm
nlp = spacy.load("de_core_news_sm")
import re

"""## Daten laden"""

def get_data():
  ds = load_dataset("ofai/GerMS-AT")
  return ds

get_data()

"""# Tokenisierung und Vektorisierung

## Tokenisierung
"""

def encode_dataset(dataset, batch_size=50):
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

  def tokenize(batch):
      return tokenizer(batch["text"], padding=True, truncation=True)

  # Daten in Datei abspeichern
  data_encoded = dataset.map(tokenize, batched=True, batch_size=batch_size)
  data_encoded.save_to_disk("data_encoded")
  return data_encoded

encode_dataset(get_data())

data_encoded = load_from_disk("data_encoded")

"""## Extract Hidden States"""

def extract_hidden_states(dataset):

  def extraction(batch):

      model_ckpt = "distilbert-base-german-cased"
      tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
      device = "cpu"
      model = AutoModel.from_pretrained(model_ckpt).to(device)

      # Modelleingaben auf der GPU platzieren
      inputs = {k:v.to(device) for k,v in batch.items()
              if k in tokenizer.model_input_names}
      # Hidden States extrahieren
      with torch.no_grad():
        last_hidden_state = model(**inputs).last_hidden_state
      # Vektor zurückgeben
      return {"hidden_state": last_hidden_state[:,0].cpu().numpy()}

  dataset.set_format("torch", columns=["input_ids", "attention_mask", "annotations"])

  data_hidden = dataset.map(extraction, batched=True, batch_size=50, desc="Processing batches")

  # Daten in Datei abspeichern
  data_hidden.save_to_disk("data_hidden_processed")
  return data_hidden

extract_hidden_states(encode_dataset(get_data()))

dateipfad = "/content/data_hidden_processed"
hidden_states = load_from_disk(dateipfad)

"""# Labels extrahieren

### Median der Label-Listen extrahieren (Multi Median Methode)
"""

def extract_numeric_labels(dataset):
  y_train = dataset["annotations"]

  y_train_numeric = []

  for sample in y_train:
      numeric_labels = [int(ann["label"].split("-")[0]) for ann in sample]  # numerisches Label extrahieren
      mean_label = round(np.mean(numeric_labels))  # Mittelwert berechnen
      y_train_numeric.append(mean_label)

  return np.array(y_train_numeric)

"""### aufgerundeten Median der Label-Listen extrahieren (Multi Roundup Methode)"""

def extract_numeric_labels_roundup(dataset):
  y_train = dataset["annotations"]

  y_train_numeric = []

  for sample in y_train:
      numeric_labels = [int(ann["label"].split("-")[0]) for ann in sample]  # numerisches Label extrahieren
      mean_label = round(np.mean(numeric_labels)+0.5)  # aufgerundeten Mittelwert berechnen
      y_train_numeric.append(mean_label)

  return np.array(y_train_numeric)

"""### Labels in binäre Klassen 0 und 1 aufteilen"""

def extract_numeric_labels_binary(dataset):
  y_train = dataset["annotations"]

  y_train_numeric = []

  for sample in y_train:
      numeric_labels = [int(ann["label"].split("-")[0]) for ann in sample]  # numerisches Label extrahieren
      mean_label = np.mean(numeric_labels)  # Mittelwert berechnen
      if mean_label > 0:
        mean_label = 1 # wenn der Mittelwert größer als 0 ist wird Label 1 gewählt
      else:
        mean_label = 0 # ansonsten Label 0
      y_train_numeric.append(mean_label)

  return np.array(y_train_numeric)

"""### Labels der individuellen Annotatoren extrahieren"""

def extract_numeric_labels_person(dataset):
  # Listen für jeden indivuduellen Annotator erstellen
  a1 = []
  a2 = []
  a3 = []
  a4 = []
  a5 = []
  a7 = []
  a8 = []
  a9 = []
  a10 = []
  a11 = []
  a12 = []
  text_a1 = []
  text_a2 = []
  text_a3 = []
  text_a4 = []
  text_a5 = []
  text_a7 = []
  text_a8 = []
  text_a9 = []
  text_a10 = []
  text_a11 = []
  text_a12 = []
  input_ids_a1 = []
  input_ids_a2 = []
  input_ids_a3 = []
  input_ids_a4 = []
  input_ids_a5 = []
  input_ids_a7 = []
  input_ids_a8 = []
  input_ids_a9 = []
  input_ids_a10 = []
  input_ids_a11 = []
  input_ids_a12 = []
  attention_mask_a1 = []
  attention_mask_a2 = []
  attention_mask_a3 = []
  attention_mask_a4 = []
  attention_mask_a5 = []
  attention_mask_a7 = []
  attention_mask_a8 = []
  attention_mask_a9 = []
  attention_mask_a10 = []
  attention_mask_a11 = []
  attention_mask_a12 = []

  for sample in dataset:
      text = sample["text"]
      annotations = sample["annotations"]  # Liste von dicts mit "user" und "label"
      input_ids = sample['input_ids']
      attention_mask = sample['attention_mask']
      for ann in annotations:
          user = ann["user"]
          label = int(ann["label"].split("-")[0]) # numerisches Label extrahieren
          if user == "A001":
              a1.append(label)
              text_a1.append(text)
              input_ids_a1.append(input_ids)
              attention_mask_a1.append(attention_mask)
          if user == "A002":
              a2.append(label)
              text_a2.append(text)
              input_ids_a2.append(input_ids)
              attention_mask_a2.append(attention_mask)
          if user == "A003":
              a3.append(label)
              text_a3.append(text)
              input_ids_a3.append(input_ids)
              attention_mask_a3.append(attention_mask)
          if user == "A004":
              a4.append(label)
              text_a4.append(text)
              input_ids_a4.append(input_ids)
              attention_mask_a4.append(attention_mask)
          if user == "A005":
              a5.append(label)
              text_a5.append(text)
              input_ids_a5.append(input_ids)
              attention_mask_a5.append(attention_mask)
          if user == "A007":
              a7.append(label)
              text_a7.append(text)
              input_ids_a7.append(input_ids)
              attention_mask_a7.append(attention_mask)
          if user == "A008":
              a8.append(label)
              text_a8.append(text)
              input_ids_a8.append(input_ids)
              attention_mask_a8.append(attention_mask)
          if user == "A009":
              a9.append(label)
              text_a9.append(text)
              input_ids_a9.append(input_ids)
              attention_mask_a9.append(attention_mask)
          if user == "A010":
              a10.append(label)
              text_a10.append(text)
              input_ids_a10.append(input_ids)
              attention_mask_a10.append(attention_mask)
          if user == "A011":
              a11.append(label)
              text_a11.append(text)
              input_ids_a11.append(input_ids)
              attention_mask_a11.append(attention_mask)
          if user == "A012":
              a12.append(label)
              text_a12.append(text)
              input_ids_a12.append(input_ids)
              attention_mask_a12.append(attention_mask)

  return text_a1,a1,input_ids_a1,attention_mask_a1, text_a2,a2,input_ids_a2,attention_mask_a2, text_a3, a3,input_ids_a3,attention_mask_a3, text_a4, a4,input_ids_a4,attention_mask_a4, text_a5,a5,input_ids_a5,attention_mask_a5, text_a7,a7,input_ids_a7,attention_mask_a7, text_a8, a8,input_ids_a8,attention_mask_a8, text_a9,a9,input_ids_a9,attention_mask_a9, text_a10, a10,input_ids_a10,attention_mask_a10, text_a11, a11,input_ids_a11,attention_mask_a11, text_a12, a12,input_ids_a12,attention_mask_a12

"""### Dictionary mit Daten der individuellen Annotatoren erstellen"""

def labels_per_annotator(dataset):

  all_outputs = extract_numeric_labels_person(dataset["train"])
  text_a1, a1, input_ids_a1, attention_mask_a1 = all_outputs[0], all_outputs[1],all_outputs[2], all_outputs[3]
  text_a2, a2, input_ids_a2, attention_mask_a2 = all_outputs[4], all_outputs[5], all_outputs[6], all_outputs[7]
  text_a3, a3, input_ids_a3, attention_mask_a3 = all_outputs[8], all_outputs[9], all_outputs[10], all_outputs[11]
  text_a4, a4, input_ids_a4, attention_mask_a4 = all_outputs[12], all_outputs[13], all_outputs[14], all_outputs[15]
  text_a5, a5, input_ids_a5, attention_mask_a5 = all_outputs[16], all_outputs[17], all_outputs[18], all_outputs[19]
  text_a7, a7, input_ids_a7, attention_mask_a7 = all_outputs[20], all_outputs[21], all_outputs[22], all_outputs[23]
  text_a8, a8, input_ids_a8, attention_mask_a8 = all_outputs[24], all_outputs[25], all_outputs[26], all_outputs[27]
  text_a9, a9, input_ids_a9, attention_mask_a9 = all_outputs[28], all_outputs[29], all_outputs[30], all_outputs[31]
  text_a10, a10, input_ids_a10, attention_mask_a10 = all_outputs[32], all_outputs[33], all_outputs[34], all_outputs[35]
  text_a11, a11, input_ids_a11, attention_mask_a11 = all_outputs[36], all_outputs[37], all_outputs[38], all_outputs[39]
  text_a12, a12, input_ids_a12, attention_mask_a12 = all_outputs[40], all_outputs[41], all_outputs[42], all_outputs[43]

  all_outputs_test = extract_numeric_labels_person(dataset["test"])
  text_a1_test, a1_test, input_ids_a1_test, attention_mask_a1_test = all_outputs_test[0], all_outputs_test[1], all_outputs_test[2], all_outputs_test[3]
  text_a2_test, a2_test, input_ids_a2_test, attention_mask_a2_test = all_outputs_test[4], all_outputs_test[5], all_outputs_test[6], all_outputs_test[7]
  text_a3_test, a3_test, input_ids_a3_test, attention_mask_a3_test = all_outputs_test[8], all_outputs_test[9], all_outputs_test[10], all_outputs_test[11]
  text_a4_test, a4_test, input_ids_a4_test, attention_mask_a4_test = all_outputs_test[12], all_outputs_test[13], all_outputs_test[14], all_outputs_test[15]
  text_a5_test, a5_test, input_ids_a5_test, attention_mask_a5_test = all_outputs_test[16], all_outputs_test[17], all_outputs_test[18], all_outputs_test[19]
  text_a7_test, a7_test, input_ids_a7_test, attention_mask_a7_test = all_outputs_test[20], all_outputs_test[21], all_outputs_test[22], all_outputs_test[23]
  text_a8_test, a8_test, input_ids_a8_test, attention_mask_a8_test = all_outputs_test[24], all_outputs_test[25], all_outputs_test[26], all_outputs_test[27]
  text_a9_test, a9_test, input_ids_a9_test, attention_mask_a9_test = all_outputs_test[28], all_outputs_test[29], all_outputs_test[30], all_outputs_test[31]
  text_a10_test, a10_test, input_ids_a10_test, attention_mask_a10_test = all_outputs_test[32], all_outputs_test[33], all_outputs_test[34], all_outputs_test[35]
  text_a11_test, a11_test, input_ids_a11_test, attention_mask_a11_test = all_outputs_test[36], all_outputs_test[37], all_outputs_test[38], all_outputs_test[39]
  text_a12_test, a12_test, input_ids_a12_test, attention_mask_a12_test = all_outputs_test[40], all_outputs_test[41], all_outputs_test[42], all_outputs_test[43]

  texts = {
      "a1": text_a1,
      "a2": text_a2,
      "a3": text_a3,
      "a4": text_a4,
      "a5": text_a5,
      "a7": text_a7,
      "a8": text_a8,
      "a9": text_a9,
      "a10": text_a10,
      "a11": text_a11,
      "a12": text_a12}

  labels = {
      "a1": a1,
      "a2": a2,
      "a3": a3,
      "a4": a4,
      "a5": a5,
      "a7": a7,
      "a8": a8,
      "a9": a9,
      "a10": a10,
      "a11": a11,
      "a12": a12}

  input_ids = {
      "a1": input_ids_a1,
      "a2": input_ids_a2,
      "a3": input_ids_a3,
      "a4": input_ids_a4,
      "a5": input_ids_a5,
      "a7": input_ids_a7,
      "a8": input_ids_a8,
      "a9": input_ids_a9,
      "a10": input_ids_a10,
      "a11": input_ids_a11,
      "a12": input_ids_a12}

  attention_mask = {
      "a1": attention_mask_a1,
      "a2": attention_mask_a2,
      "a3": attention_mask_a3,
      "a4": attention_mask_a4,
      "a5": attention_mask_a5,
      "a7": attention_mask_a7,
      "a8": attention_mask_a8,
      "a9": attention_mask_a9,
      "a10": attention_mask_a10,
      "a11": attention_mask_a11,
      "a12": attention_mask_a12}

  texts_test = {
      "a1": text_a1_test,
      "a2": text_a2_test,
      "a3": text_a3_test,
      "a4": text_a4_test,
      "a5": text_a5_test,
      "a7": text_a7_test,
      "a8": text_a8_test,
      "a9": text_a9_test,
      "a10": text_a10_test,
      "a11": text_a11_test,
      "a12": text_a12_test}

  labels_test = {
      "a1": a1_test,
      "a2": a2_test,
      "a3": a3_test,
      "a4": a4_test,
      "a5": a5_test,
      "a7": a7_test,
      "a8": a8_test,
      "a9": a9_test,
      "a10": a10_test,
      "a11": a11_test,
      "a12": a12_test}

  input_ids_test = {
      "a1": input_ids_a1_test,
      "a2": input_ids_a2_test,
      "a3": input_ids_a3_test,
      "a4": input_ids_a4_test,
      "a5": input_ids_a5_test,
      "a7": input_ids_a7_test,
      "a8": input_ids_a8_test,
      "a9": input_ids_a9_test,
      "a10": input_ids_a10_test,
      "a11": input_ids_a11_test,
      "a12": input_ids_a12_test}

  attention_mask_test = {
      "a1": attention_mask_a1_test,
      "a2": attention_mask_a2_test,
      "a3": attention_mask_a3_test,
      "a4": attention_mask_a4_test,
      "a5": attention_mask_a5_test,
      "a7": attention_mask_a7_test,
      "a8": attention_mask_a8_test,
      "a9": attention_mask_a9_test,
      "a10": attention_mask_a10_test,
      "a11": attention_mask_a11_test,
      "a12": attention_mask_a12_test}



  # DatasetDict erstellen
  datasets_per_annotator = {}

  for key in texts:
      datasets_per_annotator[key] = DatasetDict({
          "train": Dataset.from_dict({
              "text": texts[key],
              "labels": labels[key],
              "input_ids": input_ids[key],
              "attention_mask": attention_mask[key]
          }),
          "test": Dataset.from_dict({
              "text": texts_test[key],
              "labels": labels_test[key],
              "input_ids": input_ids_test[key],
              "attention_mask": attention_mask_test[key]
          })
      })

  # Daten in Datei abspeichern
  for name, ds_dict in datasets_per_annotator.items():
    ds_dict.save_to_disk(f"datasets_per_annotator/{name}")

  return datasets_per_annotator

labels_per_annotator(data_encoded)

annotator_names = ["a1", "a2", "a3", "a4", "a5", "a7", "a8", "a9", "a10", "a11", "a12"]

datasets_per_annotator = {
    name: load_from_disk(f"datasets_per_annotator/{name}") for name in annotator_names}

"""### Annotator Labels visualisieren"""

all_data = []

for annotator, ds in datasets_per_annotator.items():
    for split in ["train", "test"]:
        if split in ds:
            labels = ds[split]["labels"]
            for label in labels:
                all_data.append({"Annotator": annotator, "Label": label, "Split": split})

df = pd.DataFrame(all_data)

# Plot erstellen
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x="Label", hue="Annotator", palette="tab10")
plt.title("Verteilung der vergebenen Labels pro Annotator")
plt.xlabel("Label")
plt.ylabel("Anzahl")
plt.legend(title="Annotator", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""## Labels zu Dataset hinzufügen"""

def add_labels(dataset):
  # Multi Median Labels zum Dataset hinzufügen
  dataset["train"] = dataset["train"].add_column("labels", extract_numeric_labels(dataset["train"]))
  dataset["test"] = dataset["test"].add_column("labels", extract_numeric_labels(dataset["test"]))

  # Multi Roundup Labels zum Dataset hinzufügen
  dataset["train"] = dataset["train"].add_column("labels_roundup", extract_numeric_labels_roundup(dataset["train"]))
  dataset["test"] = dataset["test"].add_column("labels_roundup", extract_numeric_labels_roundup(dataset["test"]))

  # Binäre Labels zum Dataset hinzufügen
  dataset["train"] = dataset["train"].add_column("labels_binary", extract_numeric_labels_binary(dataset["train"]))
  dataset["test"] = dataset["test"].add_column("labels_binary", extract_numeric_labels_binary(dataset["test"]))

  return dataset

add_labels(hidden_states)

"""### überflüssige Spalten des Datasets löschen"""

dataset_binary = hidden_states.remove_columns(["id", "annotations", "round", "source", "labels", "labels_roundup"])
dataset_binary = dataset_binary.rename_column("labels_binary", "labels")

dataset_multi = hidden_states.remove_columns(["id", "annotations", "round", "source", "labels_binary", "labels_roundup"])

dataset_multi_roundup = hidden_states.remove_columns(["id", "annotations", "round", "source", "labels_binary", "labels"])
dataset_multi_roundup = dataset_multi_roundup.rename_column("labels_roundup", "labels")

print(dataset_multi.column_names)
print(dataset_multi_roundup.column_names)
print(dataset_binary.column_names)

"""# linguistische Analyse der sexistisch gelabelten Kommentare

### Alle Kommentare mit Label 4 extrahieren
"""

dataset_binary.reset_format()
dataset_multi.reset_format()
dataset_multi_roundup.reset_format()

# Alle Kommentare mit Label 4 aus beiden Splits holen (Multi Median)
texts_train = [sample["text"] for sample in dataset_multi["train"] if sample["labels"] == 4]
texts_test = [sample["text"] for sample in dataset_multi["test"] if sample["labels"] == 4]
all_texts = texts_train + texts_test
print("Anzahl Kommentare mit Label 4: ", len(all_texts))

# Daten in Datei abspeichern
with open("label_4.txt", "w", encoding="utf-8") as f:
    for text in all_texts:
        f.write(text.strip() + "\n")

# Alle Kommentare mit Label 4 aus beiden Splits holen (Multi Roundup)
texts_train = [sample["text"] for sample in dataset_multi_roundup["train"] if sample["labels"] == 4]
texts_test = [sample["text"] for sample in dataset_multi_roundup["test"] if sample["labels"] == 4]
all_texts = texts_train + texts_test
print("Anzahl Kommentare mit Label 4: ", len(all_texts))

# Daten in Datei abspeichern
with open("label_4_roundup.txt", "w", encoding="utf-8") as f:
    for text in all_texts:
        f.write(text.strip() + "\n")

"""### Die häufigsten Worte aus den Label 4 Kommentaren herausfiltern"""

with open("label_4_roundup.txt", "r", encoding="utf-8") as file:
    content = file.read()

text = re.sub(r"[^a-zA-Z0-9\s]", "", content).lower()
words = text.split()

# Wörter zählen
wordcount = Counter(words)

# Top-Wörter ausgeben
print("Häufigste Wörter:")
for word, count in wordcount.most_common(10): # Top 10
  print(f"{word}: {count}")
print("\n")

doc = nlp(content)

# Nomen extrahieren
nouns = [token.lemma_ for token in doc if token.pos_ in ("NOUN")]

# Adjektive extrahieren
adj = [token.lemma_ for token in doc if token.pos_ in ("ADJ")]

# Adverben extrahieren
adv = [token.lemma_ for token in doc if token.pos_ in ("ADV")]

# Verben extrahieren
verb = [token.lemma_ for token in doc if token.pos_ in ("VERB")]

nounlist = []
nouncount = Counter(nouns)
print("Häufigste Nomen:")
for noun, count in nouncount.most_common(30): # Top 30
  print(f"{noun}: {count}")
  nounlist.append(noun)
print("\n")

adjlist = []
adjcount = Counter(adj)
print("Häufigste Adjektive:")
for adj, count in adjcount.most_common(30): # Top 30
  print(f"{adj}: {count}")
  adjlist.append(adj)
print("\n")

advcount = Counter(adv)
print("Häufigste Adverben:")
for adv, count in advcount.most_common(5): # Top 5
  print(f"{adv}: {count}")
print("\n")

verbcount = Counter(verb)
print("Häufigste Verben:")
for verb, count in verbcount.most_common(5): # Top 5
  print(f"{verb}: {count}")
print("\n")

with open("nounlist.txt", "w", encoding="utf-8") as f:
    for word in nounlist:
        f.write(word.strip() + "\n")

with open("adjlist.txt", "w", encoding="utf-8") as f:
    for word in adjlist:
        f.write(word.strip() + "\n")

"""# Wortliste mit frauenfeindlichen Begriffen generieren

Wortliste von ChatGPT
"""

# Dateipfad muss gegebenenfalls angepasst werden, je nachdem wo die Dateie abgespeichert ist.
# Sie kann im GitHub Ordner heruntergeladen werden.
path = '/content/wordlist_chatgpt.txt'
with open(path, "r", encoding="utf-8") as f:
  wordlist_gpt = [zeile.strip() for zeile in f]

"""Wortliste von Google Gemini

"""

# Dateipfad muss gegebenenfalls angepasst werden, je nachdem wo die Dateie abgespeichert ist.
# Sie kann im GitHub Ordner heruntergeladen werden.
path = '/content/wordlist_gemini.txt'
with open(path, "r", encoding="utf-8") as f:
   lines = f.readlines()
wordlist_gemini = list(dict.fromkeys([line.strip() for line in lines if line.strip()]))

"""Wortliste von Grok"""

# Dateipfad muss gegebenenfalls angepasst werden, je nachdem wo die Dateie abgespeichert ist.
# Sie kann im GitHub Ordner heruntergeladen werden.
path = '/content/wordlist_grok.txt'
with open(path, "r", encoding="utf-8") as f:
   lines = f.readlines()
wordlist_grok = list(dict.fromkeys([line.strip() for line in lines if line.strip()]))

# Listen zusammenführen und Duplikate entfernen
wordlist_complete = list(set(wordlist_gpt + wordlist_gemini + wordlist_grok))
print(wordlist_complete)
print("Länge der zusammengefügten Wortliste: ", len(wordlist_complete))

"""Ermitteln der Häufigkeit, mit der die Begriffe aus der Wortliste in den Kommentaren vorkommen, sortiert nach vergebenem Label"""

def contains_sexist_word(dataset, sexist_words):
  # Begriffe, die hauptsächlich in als nicht sexistisch gelabelten Kommentaren auftreten entfernen
  delete = ['dumm', 'heiß','nörgelnd', 'ausgedient', 'begriffsstutzig', 'frigid', 'gehört an den herd', 'nur eine frau', 'biest', 'jagen', 'püppchen', 'tippse', 'wertlos', 'heulsuse', 'nutzlos', 'spielzeug', 'vitamin b', 'zähmen', 'plage', 'beleidigte leberwurst']
  sexist_words_filtered = [word for word in sexist_words if word not in delete]
  print(sexist_words_filtered)

  count = 0
  label_0_words = []
  label_1_words = []
  label_2_words = []
  label_3_words = []
  label_4_words = []

  label_0 = 0
  label_1 = 0
  label_2 = 0
  label_3 = 0
  label_4 = 0

  for sample in dataset:
    text = sample["text"]
    label = sample["labels"]
    text = text.lower()
    for word in sexist_words_filtered:
      if word in text:
        count += 1
        if label == 0:
          label_0 += 1
          label_0_words.append(word)
        if label == 1:
          label_1 += 1
          label_1_words.append(word)
        if label == 2:
          label_2 += 1
          label_2_words.append(word)
        if label == 3:
          label_3 += 1
          label_3_words.append(word)
        if label == 4:
          label_4 += 1
          label_4_words.append(word)

  print("COUNT = ", count)
  print("LABEL 0 Count = ", label_0)
  print("LABEL 1 Count = ", label_1)
  print("LABEL 2 Count = ", label_2)
  print("LABEL 3 Count = ", label_3)
  print("LABEL 4 Count = ", label_4, "\n")
  false0 = Counter(label_0_words)
  print("häufigste Label 0 Worte:", false0)
  false1 = Counter(label_1_words)
  print("häufigste Label 1 Worte:", false1)
  false2 = Counter(label_2_words)
  print("häufigste Label 2 Worte:", false2)
  false3 = Counter(label_3_words)
  print("häufigste Label 3 Worte:", false3)
  false4 = Counter(label_4_words)
  print("häufigste Label 4 Worte:", false4, "\n")

  # alle Listen in Mengen umwandeln
  set1 = set(false0)
  set2 = set(false0 + false1)
  set_rest = set(false1 + false2 + false3 + false4)
  set_high = set(false2 + false3 + false4)

  # Wörter, die nur in liste1 vorkommen identifizieren
  only_in_list1 = set1 - set_rest

  # Wörter, die nur in liste1 und liste2 vorkommen identifizieren
  only_in_list1_2 = set2 - set_high

  print("nur in der Label 0 Liste: ", only_in_list1, "Länge: " , len(only_in_list1))
  print("nur in der Label 0 und 1 Liste: ", only_in_list1_2, "Länge: " , len(only_in_list1_2))
  print("Länge der delete-Liste: ", len(delete))
  print("Länge der gefilterten Wortliste: ", len(sexist_words_filtered))


  counter = Counter({
    'label0': label_0,
    'label1': label_1,
    'label2': label_2,
    'label3': label_3,
    'label4': label_4})

  # Einzelwerte
  klasse1_count = counter['label0']
  rest_count = sum(counter[k] for k in ['label1', 'label2', 'label3', 'label4'])
  gesamt = klasse1_count + rest_count


  # Prozentwerte berechnen
  prozent_klasse1 = (klasse1_count / gesamt) * 100 if gesamt > 0 else 0
  prozent_rest = (rest_count / gesamt) * 100 if gesamt > 0 else 0

  # Ausgabe
  print(f"Label 0: {prozent_klasse1:.2f}%")
  print(f"Label 1–5: {prozent_rest:.2f}%")

  with open("sexist_words_filtered.txt", "w", encoding="utf-8") as f:
    for word in sexist_words_filtered:
        f.write(word.strip() + "\n")

contains_sexist_word(dataset_multi_roundup["train"], wordlist_complete)

# verbesserte Wortliste in Datei abspeichern
with open("sexist_words_filtered.txt", "r", encoding="utf-8") as file:
    sexist_words_filtered = file.read()
    sexist_words_filtered = list(sexist_words_filtered.split("\n"))
    sexist_words_filtered = [word for word in sexist_words_filtered if word.strip() != '']

"""## Neue Datasets mit "has_sexist_word" Feature erstellen"""

dataset_binary_newfeature = dataset_binary
dataset_multi_newfeature = dataset_multi
dataset_multi_roundup_newfeature = dataset_multi_roundup

def sexist_word(text, sexist_words):
    return int(any(word in text.lower() for word in sexist_words))

print(len(dataset_binary["train"]))
print(len(dataset_binary["test"]))

dataset_binary_newfeature["train"] = dataset_binary_newfeature["train"].map(lambda x: {"has_sexist_word": sexist_word(x["text"], sexist_words_filtered)})
dataset_binary_newfeature["test"] = dataset_binary_newfeature["test"].map(lambda x: {"has_sexist_word": sexist_word(x["text"], sexist_words_filtered)})

dataset_multi_newfeature["train"] = dataset_multi_newfeature["train"].map(lambda x: {"has_sexist_word": sexist_word(x["text"], sexist_words_filtered)})
dataset_multi_newfeature["test"] = dataset_multi_newfeature["test"].map(lambda x: {"has_sexist_word": sexist_word(x["text"], sexist_words_filtered)})

dataset_multi_roundup_newfeature["train"] = dataset_multi_roundup_newfeature["train"].map(lambda x: {"has_sexist_word": sexist_word(x["text"], sexist_words_filtered)})
dataset_multi_roundup_newfeature["test"] = dataset_multi_roundup_newfeature["test"].map(lambda x: {"has_sexist_word": sexist_word(x["text"], sexist_words_filtered)})

dataset_binary = dataset_binary.remove_columns("has_sexist_word")
dataset_multi = dataset_multi.remove_columns("has_sexist_word")
dataset_multi_roundup = dataset_multi_roundup.remove_columns("has_sexist_word")

print(len(dataset_binary_newfeature["train"]))
print(len(dataset_binary_newfeature["test"]))

print(dataset_binary["test"].column_names)

print(dataset_binary_newfeature["test"].column_names)

print(dataset_multi_newfeature["test"].column_names)

"""### Label Counter für die verschiedenen Datasets"""

train_labels = [example["labels"] for example in dataset_binary["train"]]
test_labels = [example["labels"] for example in dataset_binary["test"]]

print("Train Labels Binary:", Counter(train_labels))
print("Test Labels Binary:", Counter(test_labels), "\n")

train_labels = [example["has_sexist_word"] for example in dataset_binary_newfeature["train"]]
test_labels = [example["has_sexist_word"] for example in dataset_binary_newfeature["test"]]

print("Train Values Binary mit Wordliste:", Counter(train_labels))
print("Test Values Binary mit Wordliste:", Counter(test_labels), "\n")

train_labels = [example["labels"] for example in dataset_multi["train"]]
test_labels = [example["labels"] for example in dataset_multi["test"]]

print("Train Labels Multi:", Counter(train_labels))
print("Test Labels Multi:", Counter(test_labels), "\n")

train_labels = [example["labels"] for example in dataset_multi_roundup["train"]]
test_labels = [example["labels"] for example in dataset_multi_roundup["test"]]

print("Train Labels Multi Roundup:", Counter(train_labels))
print("Test Labels Multi Roundup:", Counter(test_labels), "\n")

train_labels = [example["labels"] for example in datasets_per_annotator["a1"]["train"]]
test_labels = [example["labels"] for example in datasets_per_annotator["a1"]["test"]]

print("Train Labels a1:", Counter(train_labels))
print("Test Labels a1:", Counter(test_labels), "\n")

train_labels = [example["labels"] for example in datasets_per_annotator["a12"]["train"]]
test_labels = [example["labels"] for example in datasets_per_annotator["a12"]["test"]]

print("Train Labels a12:", Counter(train_labels))
print("Test Labels a12:", Counter(test_labels), "\n")

"""# Logistische Regression mit TF-IDF

Die drei verschiedenen Datensätze können jeweils durch Aus- und Einkommentieren der Zeilen für die logistische Regression verwendet werden.
"""

texts_train = dataset_binary["train"]["text"]
labels_train = dataset_binary["train"]["labels"]
texts_test = dataset_binary["test"]["text"]
labels_test = dataset_binary["test"]["labels"]

#texts_train = dataset_multi["train"]["text"]
#labels_train = dataset_multi["train"]["labels"]
#texts_test = dataset_multi["test"]["text"]
#labels_test = dataset_multi["test"]["labels"]

#texts_train = dataset_multi_roundup["train"]["text"]
#labels_train = dataset_multi_roundup["train"]["labels"]
#texts_test = dataset_multi_roundup["test"]["text"]
#labels_test = dataset_multi_roundup["test"]["labels"]

german_stopwords = stopwords.words('german')

vectorizer_tfidf = TfidfVectorizer(
                      stop_words=german_stopwords,  # automatische Stoppwort-Filterung
                      max_df=0.8,            # ignoriert sehr häufige Wörter
                      min_df=2,              # ignoriert sehr seltene Wörter
                      ngram_range=(1,2)     # auch 2-Wort-Kombis
                      )

"""### Logistische Regression mit TF-IDF

"""

# Matrix erstellen
X_train = vectorizer_tfidf.fit_transform(texts_train)
X_test = vectorizer_tfidf.transform(texts_test)

# Als DataFrame anzeigen
tfidf_df = pd.DataFrame(X_train.toarray(), columns=vectorizer_tfidf.get_feature_names_out())

model = LogisticRegression(max_iter=1000)
model.fit(X_train, labels_train)

# Vorhersage & Evaluation
labels_pred = model.predict(X_test)
print(classification_report(labels_test, labels_pred, digits=3))

"""### Confusion Matrix"""

cm = confusion_matrix(labels_test, labels_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

"""# Logistische Regression mit Transformer-Embeddings

## Featurematrix und Klassifizierer erstellen
"""

def featurematrix(dataset):

  # auskommentieren je nach Label-Anwendung

  # binär
  y_train = dataset_binary["train"]["labels"]
  y_test = dataset_binary["test"]["labels"]

  # multi Median
  #y_train = dataset_multi["train"]["labels"]
  #y_test = dataset_multi["test"]["labels"]

  # multi roundup
  #y_train = dataset_multi_roundup["train"]["labels"]
  #y_test = dataset_multi_roundup["test"]["labels"]

  X_train = np.array(dataset["train"]["hidden_state"])
  X_test = np.array(dataset["test"]["hidden_state"])

  lr_clf = LogisticRegression(class_weight='balanced', max_iter=10000)

  lr_clf.fit(X_train, y_train)

  y_pred = lr_clf.predict(X_test)
  score = lr_clf.score(X_test, y_test)
  score_f1 = f1_score(y_test, y_pred, average='macro')

  return X_train, y_train, X_test, y_test, score, score_f1

featurematrix(hidden_states)

"""## Konfusionsmatrix erstellen

### Konfusionsmatrix für binäre Labels
"""

def matrix(X_train, y_train, X_test, y_test):

  def plot_confusion_matrix(y_preds, y_true, labels):
      cm = confusion_matrix(y_true, y_preds, labels=[0, 1], normalize='true')
      fig, ax = plt.subplots(figsize=(6,6))
      disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
      disp.plot(cmap="Blues", values_format=".2f", ax=ax, colorbar=False)
      plt.title("Normalisierte Konfusionsmatrix binär")
      plt.show()

  lr_clf = LogisticRegression(class_weight='balanced', max_iter=10000)
  lr_clf.fit(X_train, y_train)
  y_preds = lr_clf.predict(X_test)
  class_labels = ["0-Kein", "1-Misogynie"]

  plot_confusion_matrix(y_preds, y_test, class_labels)

result = featurematrix(hidden_states)
X_train = result[0]
y_train = result[1]
X_test = result[2]
y_test = result[3]
score = result[4]

matrix(X_train, y_train, X_test, y_test)

print("\n" + "SCORE:")
print(score)

"""### Konfusionsmatrix für multi Labels"""

def matrix(X_train, y_train, X_test, y_test):

  def plot_confusion_matrix(y_preds, y_true, labels):
      cm = confusion_matrix(y_true, y_preds, labels=[0, 1, 2, 3, 4],normalize='true')
      fig, ax = plt.subplots(figsize=(6, 6))
      disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
      disp.plot(cmap="Blues", values_format=".2f", ax=ax, colorbar=False)
      plt.title("Normalisierte Konfusionsmatrix multi Median")
      plt.show()

  lr_clf = LogisticRegression(class_weight='balanced', max_iter=10000)
  lr_clf.fit(X_train, y_train)
  y_preds = lr_clf.predict(X_test)
  class_labels = ["0-Kein", "1-Gering", "2-Vorhanden", "3-Stark", "4-Extrem"]

  plot_confusion_matrix(y_preds, y_test, class_labels)

result = featurematrix(hidden_states)
X_train = result[0]
y_train = result[1]
X_test = result[2]
y_test = result[3]
score = result[4]

matrix(X_train, y_train, X_test, y_test)

print("\n" + "SCORE:")
print(score)

"""# Fine-Tuning von BERT-Modellen"""

os.environ["WANDB_DISABLED"] = "true"

"""### Validation Sets für die verschiedenen Datasets erstellen"""

def create_train_val_test(dataset, val_size=0.1, seed=42):

    # Train/Validation Split erzeugen
    split = dataset["train"].train_test_split(test_size=val_size, seed=seed)

    # Neues DatasetDict bauen
    dataset_final = DatasetDict({
        "train": split["train"],
        "validation": split["test"],
        "test": dataset["test"]
    })

    return dataset_final

# Validation Set für die Datasets ohne Zusatzfeature erstellen
dataset_binary_split = create_train_val_test(dataset_binary)
dataset_multi_split = create_train_val_test(dataset_multi)
dataset_multi_roundup_split = create_train_val_test(dataset_multi_roundup)

# Validation Set für die Datasets mit Zusatzfeature erstellen
dataset_binary_newfeature_split = create_train_val_test(dataset_binary_newfeature)
dataset_multi_newfeature_split = create_train_val_test(dataset_multi_newfeature)
dataset_multi_roundup_newfeature_split = create_train_val_test(dataset_multi_roundup_newfeature)

# Neues Dictionary für Annotatoren-Datasets mit Validation-Set
datasets_per_annotator_split = {}

for annotator, ds in datasets_per_annotator.items():
    datasets_per_annotator_split[annotator] = create_train_val_test(ds, val_size=0.1, seed=42)

"""## Training"""

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="macro")
    acc = accuracy_score(labels, preds)
    prec = precision_score(labels, preds, average="macro", zero_division=0)
    rec = recall_score(labels, preds, average="macro", zero_division=0)
    return {"accuracy": acc, "f1-macro": f1, "precision": prec, "recall": rec}

"""### BERT Modell für binäre Labels trainieren

"""

def model_training_binary(dataset):
  num_labels = 2
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  config = AutoConfig.from_pretrained(model_ckpt,
                                    num_labels=num_labels,
                                    hidden_dropout_prob=0.3,        # Feedforward-Dropout
                                    attention_probs_dropout_prob=0.3)  # Attention-Dropout
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  model = (AutoModelForSequenceClassification
          .from_pretrained(model_ckpt, config=config)
          .to(device))

  batch_size = 16
  training_args = TrainingArguments(
                                    num_train_epochs=3,
                                    learning_rate=3.08e-5,
                                    per_device_train_batch_size=batch_size,
                                    per_device_eval_batch_size=batch_size,
                                    weight_decay=0.01,
                                    eval_strategy="epoch",
                                    disable_tqdm=False,
                                    logging_steps=10,
                                    push_to_hub=False,
                                    log_level="error")

  trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset["train"],
                  eval_dataset=dataset["validation"],
                  tokenizer=tokenizer)

  # Training + Validation
  trainer.train();

  # Finale Evaluation auf den Testdaten
  eval_result = trainer.evaluate(eval_dataset=dataset["test"]);
  print("Finale Evaluation auf Testdaten:\n")
  for key, value in eval_result.items():
    print(f"{key}: {value:.4f}")

model_training_binary(dataset_binary_split)

"""### BERT Modell für Multi Median Labels trainieren

"""

def model_training_multi(dataset):
  num_labels = 5
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  config = AutoConfig.from_pretrained(model_ckpt,
                                    num_labels=num_labels,
                                    hidden_dropout_prob=0.3,        # Feedforward-Dropout
                                    attention_probs_dropout_prob=0.3)  # Attention-Dropout
  model = (AutoModelForSequenceClassification
          .from_pretrained(model_ckpt, config = config)
          .to(device))

  data_encoded = load_from_disk("data_encoded")
  batch_size = 16
  training_args = TrainingArguments(num_train_epochs=4,
                                    learning_rate=4.74e-5,
                                    per_device_train_batch_size=batch_size,
                                    per_device_eval_batch_size=batch_size,
                                    weight_decay=0.01,
                                    eval_strategy="epoch",
                                    disable_tqdm=False,
                                    logging_steps=10,
                                    push_to_hub=False,
                                    log_level="error")
  trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset["train"],
                  eval_dataset=dataset["validation"],
                  tokenizer=tokenizer)

  # Training + Validation
  trainer.train();

  # Finale Evaluation auf den Testdaten
  eval_result = trainer.evaluate(eval_dataset=dataset["test"]);
  print("Finale Evaluation auf Testdaten:\n")
  for key, value in eval_result.items():
    print(f"{key}: {value:.4f}")

model_training_multi(dataset_multi_split)

"""### BERT Modell für Multi Roundup Labels trainieren

"""

def model_training_multi(dataset):
  num_labels = 5
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  config = AutoConfig.from_pretrained(model_ckpt,
                                    num_labels=num_labels,
                                    hidden_dropout_prob=0.3,        # Feedforward-Dropout
                                    attention_probs_dropout_prob=0.3)  # Attention-Dropout
  model = (AutoModelForSequenceClassification
          .from_pretrained(model_ckpt, config = config)
          .to(device))

  data_encoded = load_from_disk("data_encoded")
  batch_size = 16
  training_args = TrainingArguments(num_train_epochs=3,
                                    learning_rate=4.98e-5,
                                    per_device_train_batch_size=batch_size,
                                    per_device_eval_batch_size=batch_size,
                                    weight_decay=0.01,
                                    eval_strategy="epoch",
                                    disable_tqdm=False,
                                    logging_steps=10,
                                    push_to_hub=False,
                                    log_level="error")
  trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset["train"],
                  eval_dataset=dataset["validation"],
                  tokenizer=tokenizer)

  # Training + Validation
  trainer.train();

  # Finale Evaluation auf den Testdaten
  eval_result = trainer.evaluate(eval_dataset=dataset["test"]);
  print("Finale Evaluation auf Testdaten:\n")
  for key, value in eval_result.items():
    print(f"{key}: {value:.4f}")

model_training_multi(dataset_multi_roundup_split)

"""### BERT Modell für individuelle Annotatoren trainieren

"""

def model_training_per_annotator(datasets):
  num_labels = 5
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

  results = []

  for annotator, dataset in datasets.items():
    print(f"\n===== Training für Annotator {annotator} =====")

    # Neues Modell für jeden Annotator
    config = AutoConfig.from_pretrained(
        model_ckpt,
        num_labels=num_labels,
        hidden_dropout_prob=0.3,
        attention_probs_dropout_prob=0.3)

    model = (AutoModelForSequenceClassification
            .from_pretrained(model_ckpt, config=config)
            .to(device))

    batch_size = 8

    training_args = TrainingArguments(
            num_train_epochs=4,
            learning_rate=3.32e-05,
            save_strategy="no",
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            weight_decay=0.01,
            eval_strategy="epoch",
            disable_tqdm=False,
            logging_steps=10,
            push_to_hub=False,
            log_level="error")

    trainer = Trainer(
            model=model,
            args=training_args,
            compute_metrics=compute_metrics,
            train_dataset=dataset["train"],
            eval_dataset=dataset["validation"],
            tokenizer=tokenizer)

    # Training + Validation
    trainer.train();

    # Finale Evaluation auf den Testdaten
    eval_result = trainer.evaluate(eval_dataset=dataset["test"]);
    print(f"\nFinale Evaluation auf Testdaten für {annotator}:\n")
    for key, value in eval_result.items():
      print(f"{key}: {value:.4f}")

    results.append({
      'Annotator': annotator,
      'Modell': 'distilbert-base-german-cased (annotator A1)',
      'Accuracy': eval_result['eval_accuracy'],
      'Precision': eval_result['eval_precision'],
      'Recall': eval_result['eval_recall'],
      'F1-Score': eval_result['eval_f1-macro'],
      'learning-rate': 3.32e-05,
      'batch-size': batch_size,
      'epochs': 4})

  return results

all_results = model_training_per_annotator(datasets_per_annotator_split)

"""## Wortlisten-Regelkorrektur

### BERT Modell für binäre Labels trainieren + Wordlisten Funktion
"""

def model_training_binary_wordlist(dataset):
    num_labels = 2
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_ckpt = "distilbert-base-german-cased"
    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
    config = AutoConfig.from_pretrained(
        model_ckpt,
        num_labels=num_labels,
        hidden_dropout_prob=0.3,
        attention_probs_dropout_prob=0.3
    )
    model = (AutoModelForSequenceClassification
             .from_pretrained(model_ckpt, config=config)
             .to(device))

    batch_size = 16
    logging_steps = 10
    training_args = TrainingArguments(
        num_train_epochs=3,
        learning_rate=3.08e-5,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        weight_decay=0.01,
        eval_strategy="epoch",
        disable_tqdm=False,
        logging_steps=logging_steps,
        push_to_hub=False,
        log_level="error"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer
    )
    trainer.train()

    # --- Vorhersagen ---
    predictions = trainer.predict(dataset["test"])
    logits = predictions.predictions
    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()
    predicted_labels = np.argmax(probs, axis=1)

    # Originaltexte und Labels
    texts = dataset["test"]["text"]
    true_labels = dataset["test"]["labels"]

    # --- Regelbasierte Korrektur nur bei Unsicherheit ---
    adjusted_preds = []
    for text, prob, pred in zip(texts, probs, predicted_labels):
        max_prob = np.max(prob)
        diff = abs(prob[0] - prob[1])

        # Bedingung: Modell unsicher
        if max_prob < 0.75 or diff < 0.1:
            if pred == 0 and any(word in text.lower() for word in sexist_words_filtered):
                adjusted_preds.append(1)  # Regel überschreibt
                continue

        # sonst normale Modellvorhersage
        adjusted_preds.append(pred)

    # --- Evaluation ---
    print("Ohne Regelkorrektur (nur Modell):\n")
    print(classification_report(true_labels, predicted_labels, digits=5))

    print("Mit Regelkorrektur bei Unsicherheit:\n")
    print(classification_report(true_labels, adjusted_preds, digits=5))

    # nebeneinander vergleichen (Precision/Recall/F1)
    metrics_model = precision_recall_fscore_support(true_labels, predicted_labels, average="macro")
    metrics_rules = precision_recall_fscore_support(true_labels, adjusted_preds, average="macro")

    print("Vergleich (Macro-Averages):")
    print(f"   Modell: Acc={accuracy_score(true_labels, predicted_labels):.4f}, "
          f"Prec={metrics_model[0]:.4f}, Rec={metrics_model[1]:.4f}, F1={metrics_model[2]:.4f}")
    print(f"   Hybrid: Acc={accuracy_score(true_labels, adjusted_preds):.4f}, "
          f"Prec={metrics_rules[0]:.4f}, Rec={metrics_rules[1]:.4f}, F1={metrics_rules[2]:.4f}")

model_training_binary_wordlist(dataset_binary_split)

"""### BERT Modell für Multi Median Labels trainieren mit Wortlisten-Regelkorrektur"""

def model_training_multimedian_wordlist(dataset):
    num_labels = 5
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_ckpt = "distilbert-base-german-cased"
    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
    config = AutoConfig.from_pretrained(
        model_ckpt,
        num_labels=num_labels,
        hidden_dropout_prob=0.3,
        attention_probs_dropout_prob=0.3
    )
    model = (AutoModelForSequenceClassification
             .from_pretrained(model_ckpt, config=config)
             .to(device))

    batch_size = 16
    logging_steps = 10
    training_args = TrainingArguments(
        num_train_epochs=4,
        learning_rate=4.74e-5,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        weight_decay=0.01,
        eval_strategy="epoch",
        disable_tqdm=False,
        logging_steps=logging_steps,
        push_to_hub=False,
        log_level="error"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer
    )
    trainer.train()

    # --- Vorhersagen auf den Testdaten ---
    predictions = trainer.predict(dataset["test"])
    logits = predictions.predictions
    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()
    predicted_labels = np.argmax(probs, axis=1)

    # Originaltexte und Labels
    texts = dataset["test"]["text"]
    true_labels = dataset["test"]["labels"]

    # --- Regelbasierte Korrektur nur bei Unsicherheit ---
    adjusted_preds = []
    for text, prob, pred in zip(texts, probs, predicted_labels):
      max_prob = np.max(prob)
      # Unterschied zwischen Top-2-Klassen berechnen
      top2 = np.sort(prob)[-2:]
      diff = top2[1] - top2[0]

      # Bedingung: Modell unsicher
      if max_prob < 0.75 or diff < 0.1:
          text_lower = text.lower()
          if any(word in text_lower for word in sexist_words_filtered):
              new_label = min(pred + 1, num_labels - 1)  # max capped bei 4
              adjusted_preds.append(new_label)
              continue

      # sonst normale Modellvorhersage
      adjusted_preds.append(pred)

    # --- Evaluation ---
    print("Ohne Regelkorrektur (nur Modell):\n")
    print(classification_report(true_labels, predicted_labels, digits=5))

    print("Mit Regelkorrektur bei Unsicherheit (+1 Regel):\n")
    print(classification_report(true_labels, adjusted_preds, digits=5))

    # nebeneinander vergleichen (Precision/Recall/F1)
    metrics_model = precision_recall_fscore_support(true_labels, predicted_labels, average="macro")
    metrics_rules = precision_recall_fscore_support(true_labels, adjusted_preds, average="macro")

    print("Vergleich (Macro-Averages):")
    print(f"   Modell: Acc={accuracy_score(true_labels, predicted_labels):.4f}, "
          f"Prec={metrics_model[0]:.4f}, Rec={metrics_model[1]:.4f}, F1={metrics_model[2]:.4f}")
    print(f"   Hybrid: Acc={accuracy_score(true_labels, adjusted_preds):.4f}, "
          f"Prec={metrics_rules[0]:.4f}, Rec={metrics_rules[1]:.4f}, F1={metrics_rules[2]:.4f}")

model_training_multimedian_wordlist(dataset_multi_split)

"""### BERT Modell für Multi Roundup Labels trainieren mit Wortlisten-Regelkorrektur"""

def model_training_multiroundup_wordlist(dataset):
    num_labels = 5
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_ckpt = "distilbert-base-german-cased"
    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
    config = AutoConfig.from_pretrained(
        model_ckpt,
        num_labels=num_labels,
        hidden_dropout_prob=0.3,
        attention_probs_dropout_prob=0.3
    )
    model = (AutoModelForSequenceClassification
             .from_pretrained(model_ckpt, config=config)
             .to(device))

    batch_size = 16
    logging_steps = 10
    training_args = TrainingArguments(
        num_train_epochs=3,
        learning_rate=4.98e-5,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        weight_decay=0.01,
        eval_strategy="epoch",
        disable_tqdm=False,
        logging_steps=logging_steps,
        push_to_hub=False,
        log_level="error"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer
    )
    trainer.train()

    # --- Vorhersagen auf den Testdaten ---
    predictions = trainer.predict(dataset["test"])
    logits = predictions.predictions
    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()
    predicted_labels = np.argmax(probs, axis=1)

    # Originaltexte und Labels
    texts = dataset["test"]["text"]
    true_labels = dataset["test"]["labels"]

    # --- Regelbasierte Korrektur nur bei Unsicherheit ---
    adjusted_preds = []
    for text, prob, pred in zip(texts, probs, predicted_labels):
      max_prob = np.max(prob)
      # Unterschied zwischen Top-2-Klassen berechnen
      top2 = np.sort(prob)[-2:]
      diff = top2[1] - top2[0]

      # Bedingung: Modell unsicher
      if max_prob < 0.75 or diff < 0.1:
          text_lower = text.lower()
          if any(word in text_lower for word in sexist_words_filtered):
              new_label = min(pred + 1, num_labels - 1)  # max capped bei 4
              adjusted_preds.append(new_label)
              continue

      # sonst normale Modellvorhersage
      adjusted_preds.append(pred)

    # --- Evaluation ---
    print("Ohne Regelkorrektur (nur Modell):\n")
    print(classification_report(true_labels, predicted_labels, digits=5))

    print("Mit Regelkorrektur bei Unsicherheit (+1 Regel):\n")
    print(classification_report(true_labels, adjusted_preds, digits=5))

    # nebeneinander vergleichen (Precision/Recall/F1)
    metrics_model = precision_recall_fscore_support(true_labels, predicted_labels, average="macro")
    metrics_rules = precision_recall_fscore_support(true_labels, adjusted_preds, average="macro")

    print("Vergleich (Macro-Averages):")
    print(f"   Modell: Acc={accuracy_score(true_labels, predicted_labels):.4f}, "
          f"Prec={metrics_model[0]:.4f}, Rec={metrics_model[1]:.4f}, F1={metrics_model[2]:.4f}")
    print(f"   Hybrid: Acc={accuracy_score(true_labels, adjusted_preds):.4f}, "
          f"Prec={metrics_rules[0]:.4f}, Rec={metrics_rules[1]:.4f}, F1={metrics_rules[2]:.4f}")

model_training_multiroundup_wordlist(dataset_multi_roundup_split)

"""## Wortliste als zusätzliches Feature

Neues DistilBERT-Modell erstellen, welches das neue "has_sexist_word" Feature im Datensatz erkennt und verarbeiten kann
"""

class DistilBertWithFeature(nn.Module):
    def __init__(self, model_name, num_labels, dropout_rate=0.3):
        super().__init__()
        self.bert = DistilBertModel.from_pretrained(model_name)
        self.feature_proj = nn.Linear(1, 8)  # projiziert das Zusatzfeature auf 8 Dimensionen
        self.dropout = nn.Dropout(dropout_rate)  # Dropout-Layer
        self.classifier = nn.Linear(self.bert.config.hidden_size + 8, num_labels)

    def forward(self, input_ids, attention_mask, has_sexist_word, labels=None):
        # BERT-Forward
        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = bert_outputs.last_hidden_state[:, 0]  # CLS-Token

        # Extra Feature
        extra_feature = self.feature_proj(has_sexist_word.unsqueeze(1).float())

        # Dropout anwenden
        pooled_output = self.dropout(pooled_output)

        # Kombinieren und Klassifizieren
        combined = torch.cat((pooled_output, extra_feature), dim=1)
        logits = self.classifier(combined)

        # Loss berechnen (falls Labels vorhanden)
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits, labels)

        return {"loss": loss, "logits": logits}

"""Der Data-Collator erstellt ein Batch, paddet automatisch die Eingabesequenzen und fügt das Zusatzlabel "has_sexist_word" wieder hinzu, damit das Modell beides zusammen verarbeiten kann"""

class DataCollatorWithFeature(DataCollatorWithPadding):
    def __call__(self, features):
        has_sexist_word = [f["has_sexist_word"] for f in features]
        for f in features:
            del f["has_sexist_word"]
        batch = super().__call__(features)
        batch["has_sexist_word"] = torch.tensor(has_sexist_word)
        return batch

"""### BERT Modell für binäre Labels trainieren mit zusätzlicher Wortliste als Feature"""

def model_training_wordlist_binary(dataset):
  num_labels = 2
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  data_collator = DataCollatorWithFeature(tokenizer)
  model = DistilBertWithFeature("distilbert-base-german-cased", num_labels=2)

  data_encoded = load_from_disk("data_encoded")
  batch_size = 16
  logging_steps = 10
  training_args = TrainingArguments(num_train_epochs=3,
                                    learning_rate=3.08e-5,
                                    per_device_train_batch_size=batch_size,
                                    per_device_eval_batch_size=batch_size,
                                    weight_decay=0.01,
                                    eval_strategy="epoch",
                                    disable_tqdm=False,
                                    logging_steps=logging_steps,
                                    push_to_hub=False,
                                    log_level="error")
  trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset["train"],
                  eval_dataset=dataset["validation"],
                  tokenizer=tokenizer, data_collator=data_collator)
  trainer.train();

  # Finale Evaluation auf den Testdaten
  eval_result = trainer.evaluate(eval_dataset=dataset["test"]);
  print("Finale Evaluation auf Testdaten:\n")
  for key, value in eval_result.items():
    print(f"{key}: {value:.4f}")

model_training_wordlist_binary(dataset_binary_newfeature_split)

"""### BERT Modell für Multi Median Labels trainieren mit zusätzlicher Wortliste als Feature"""

def model_training_wordlist_multi(dataset):
  num_labels = 5
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  data_collator = DataCollatorWithFeature(tokenizer)
  model = DistilBertWithFeature("distilbert-base-german-cased", num_labels=5)

  data_encoded = load_from_disk("data_encoded")
  batch_size = 16
  logging_steps = 10
  training_args = TrainingArguments(num_train_epochs=4,
                                    learning_rate=4.74e-5,
                                    per_device_train_batch_size=batch_size,
                                    per_device_eval_batch_size=batch_size,
                                    weight_decay=0.01,
                                    eval_strategy="epoch",
                                    disable_tqdm=False,
                                    logging_steps=logging_steps,
                                    push_to_hub=False,
                                    log_level="error")
  trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset["train"],
                  eval_dataset=dataset["validation"],
                  tokenizer=tokenizer, data_collator=data_collator)
  trainer.train();

  # Finale Evaluation auf den Testdaten
  eval_result = trainer.evaluate(eval_dataset=dataset["test"]);
  print("Finale Evaluation auf Testdaten:\n")
  for key, value in eval_result.items():
    print(f"{key}: {value:.4f}")

model_training_wordlist_multi(dataset_multi_newfeature_split)

"""### BERT Modell für Multi Roundup Labels trainieren mit zusätzlicher Wortliste als Feature"""

def model_training_wordlist_multiroundup(dataset):
  num_labels = 5
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  data_collator = DataCollatorWithFeature(tokenizer)
  model = DistilBertWithFeature("distilbert-base-german-cased", num_labels=5)

  data_encoded = load_from_disk("data_encoded")
  batch_size = 16
  logging_steps = 10
  training_args = TrainingArguments(num_train_epochs=3,
                                    learning_rate=4.98e-5,
                                    #learning_rate=3e-5,
                                    per_device_train_batch_size=batch_size,
                                    per_device_eval_batch_size=batch_size,
                                    weight_decay=0.01,
                                    eval_strategy="epoch",
                                    disable_tqdm=False,
                                    logging_steps=logging_steps,
                                    push_to_hub=False,
                                    log_level="error")
  trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset["train"],
                  eval_dataset=dataset["validation"],
                  tokenizer=tokenizer, data_collator=data_collator)
  trainer.train();

  # Finale Evaluation auf den Testdaten
  eval_result = trainer.evaluate(eval_dataset=dataset["test"]);
  print("Finale Evaluation auf Testdaten:\n")
  for key, value in eval_result.items():
    print(f"{key}: {value:.4f}")

model_training_wordlist_multiroundup(dataset_multi_roundup_newfeature_split)

"""## Parameter Optimierung mit Optuna

### Parameter Optimierung mit Optuna für den binären Datensatz
"""

def model_training_optuna_binary(trial):
  num_labels = 2
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  config = AutoConfig.from_pretrained(model_ckpt,
                                     num_labels=num_labels,
                                     hidden_dropout_prob=0.3,        # Feedforward-Dropout
                                     attention_probs_dropout_prob=0.3)  # Attention-Dropout
  model = (AutoModelForSequenceClassification
          .from_pretrained(model_ckpt, config = config)
          .to(device))

  learning_rate = trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True)
  batch_size = trial.suggest_categorical("batch_size", [8, 16, 32])
  num_epochs = trial.suggest_int("num_train_epochs", 2, 4)

  training_args = TrainingArguments(
      eval_strategy="epoch",
      save_strategy="no",
      learning_rate=learning_rate,
      per_device_train_batch_size=batch_size,
      per_device_eval_batch_size=batch_size,
      num_train_epochs=num_epochs,
      weight_decay=0.01,
      disable_tqdm=False,
      logging_steps=10,
      push_to_hub=False,
      log_level="error"
  )


  trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset_binary_split["train"],
                  eval_dataset=dataset_binary_split["validation"],
                  tokenizer=tokenizer)

  # Training + Validation
  trainer.train();

  # Finale Evaluation
  eval_result = trainer.evaluate(eval_dataset=dataset_binary_split["validation"]);

  print(eval_result)
  return eval_result["eval_f1-macro"]

study = optuna.create_study(direction="maximize")  # Maximieren von F1
study.optimize(model_training_optuna_binary, n_trials=20)  # 20 verschiedene Kombinationen testen

print("Beste Hyperparameter:", study.best_params)

"""### Parameter Optimierung mit Optuna für die Mehrklassen-Datensätze"""

def model_training_optuna_multi(trial):
  num_labels = 5
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  config = AutoConfig.from_pretrained(model_ckpt,
                                     num_labels=num_labels,
                                     hidden_dropout_prob=0.3,        # Feedforward-Dropout
                                     attention_probs_dropout_prob=0.3)  # Attention-Dropout
  model = (AutoModelForSequenceClassification
          .from_pretrained(model_ckpt, config = config)
          .to(device))

  learning_rate = trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True)
  batch_size = trial.suggest_categorical("batch_size", [8, 16, 32])
  num_epochs = trial.suggest_int("num_train_epochs", 2, 4)

  training_args = TrainingArguments(
      eval_strategy="epoch",
      save_strategy="no",
      learning_rate=learning_rate,
      per_device_train_batch_size=batch_size,
      per_device_eval_batch_size=batch_size,
      num_train_epochs=num_epochs,
      weight_decay=0.01,
      disable_tqdm=False,
      logging_steps=10,
      push_to_hub=False,
      log_level="error"
  )

  # Datensätze je nach Anwendung in dataset_multi_split oder dataset_multi_roundup_split setzen
  trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=dataset_multi_split["train"],
                  eval_dataset=dataset_multi_split["validation"],
                  tokenizer=tokenizer)

  # Training + Validation
  trainer.train();

  # Finale Evaluation
  # Datensatz je nach Anwendung in dataset_multi_split oder dataset_multi_roundup_split setzen
  eval_result = trainer.evaluate(eval_dataset=dataset_multi_split["validation"]);

  print(eval_result)
  return eval_result["eval_f1-macro"]

study = optuna.create_study(direction="maximize")  # Maximieren von F1
study.optimize(model_training_optuna_multi, n_trials=20)  # 20 verschiedene Kombinationen testen

print("Beste Hyperparameter:", study.best_params)

"""### Parameter Optimierung mit Optuna für die individuellen Annotatoren"""

def model_training_optuna_annotators(trial):
  num_labels = 5
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model_ckpt = "distilbert-base-german-cased"
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  config = AutoConfig.from_pretrained(model_ckpt,
                                     num_labels=num_labels,
                                     hidden_dropout_prob=0.3,        # Feedforward-Dropout
                                     attention_probs_dropout_prob=0.3)  # Attention-Dropout

  learning_rate = trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True)
  batch_size = trial.suggest_categorical("batch_size", [8, 16, 32])
  num_epochs = trial.suggest_int("num_train_epochs", 2, 4)

  training_args = TrainingArguments(
      eval_strategy="epoch",
      learning_rate=learning_rate,
      per_device_train_batch_size=batch_size,
      per_device_eval_batch_size=batch_size,
      num_train_epochs=num_epochs,
      weight_decay=0.01,
      disable_tqdm=False,
      logging_steps=10,
      push_to_hub=False,
      log_level="error"
  )

  scores = []

  for annotator, dataset in datasets_per_annotator_split.items():
      model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)
      trainer = Trainer(
          model=model,
          args=training_args,
          compute_metrics=compute_metrics,
          train_dataset=dataset["train"],
          eval_dataset=dataset["validation"],
          tokenizer=tokenizer
      )

      trainer.train()
      eval_result = trainer.evaluate(dataset["test"])
      scores.append(eval_result["eval_f1-macro"])

  # Mittelwert der F1-Scores über alle Annotatoren
  return float(np.mean(scores))

study = optuna.create_study(direction="maximize")  # Maximieren von F1
study.optimize(model_training_optuna_annotators, n_trials=10) # 10 verschiedene Kombinationen testen

print("Beste Hyperparameter:", study.best_params)